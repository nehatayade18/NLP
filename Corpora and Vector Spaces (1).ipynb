{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora and Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import open  # for transparently opening remote files\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import numpy as np\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Strings to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "# Map each word in corpus to a unique integer ID\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "# Token : integer ID mapping\n",
    "pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [ dictionary.doc2bow(text) for text in texts ]\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Streaming â€“ One Document at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('https://raw.githubusercontent.com/RaRe-Technologies/gensim/develop/docs/notebooks/datasets/mycorpus.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate corpus object\n",
    "corpus_memory_friendly = MyCorpus()\n",
    "\n",
    "# load one vector at a time into memory\n",
    "for vector in corpus_memory_friendly:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 8,\n",
      " 'abc': 0,\n",
      " 'and': 19,\n",
      " 'applications': 1,\n",
      " 'binary': 27,\n",
      " 'computer': 2,\n",
      " 'engineering': 20,\n",
      " 'eps': 16,\n",
      " 'error': 22,\n",
      " 'for': 3,\n",
      " 'generation': 28,\n",
      " 'graph': 32,\n",
      " 'human': 4,\n",
      " 'in': 33,\n",
      " 'interface': 5,\n",
      " 'intersection': 34,\n",
      " 'iv': 36,\n",
      " 'lab': 6,\n",
      " 'machine': 7,\n",
      " 'management': 17,\n",
      " 'measurement': 23,\n",
      " 'minors': 37,\n",
      " 'of': 9,\n",
      " 'opinion': 10,\n",
      " 'ordering': 38,\n",
      " 'paths': 35,\n",
      " 'perceived': 24,\n",
      " 'quasi': 39,\n",
      " 'random': 29,\n",
      " 'relation': 25,\n",
      " 'response': 11,\n",
      " 'survey': 12,\n",
      " 'system': 13,\n",
      " 'testing': 21,\n",
      " 'the': 18,\n",
      " 'time': 14,\n",
      " 'to': 26,\n",
      " 'trees': 30,\n",
      " 'unordered': 31,\n",
      " 'user': 15,\n",
      " 'well': 40,\n",
      " 'widths': 41}\n"
     ]
    }
   ],
   "source": [
    "# construct the dictionary without loading all texts into memory\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('https://raw.githubusercontent.com/RaRe-Technologies/gensim/develop/docs/notebooks/datasets/mycorpus.txt'))\n",
    "\n",
    "# Token : integer ID mapping\n",
    "pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords and words that occur only once\n",
    "stop_ids =  [ dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id ]\n",
    "once_ids = [ tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1 ]\n",
    "\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-23 13:55:43,890 : INFO : storing corpus in Matrix Market format to corpus.mm\n",
      "2020-06-23 13:55:43,895 : INFO : saving sparse matrix to corpus.mm\n",
      "2020-06-23 13:55:43,898 : INFO : PROGRESS: saving document #0\n",
      "2020-06-23 13:55:43,900 : INFO : saved 2x2 matrix, density=25.000% (1/4)\n",
      "2020-06-23 13:55:43,905 : INFO : saving MmCorpus index to corpus.mm.index\n",
      "2020-06-23 13:55:43,909 : INFO : converting corpus to SVMlight format: corpus.svmlight\n",
      "2020-06-23 13:55:43,914 : INFO : saving SvmLightCorpus index to corpus.svmlight.index\n",
      "2020-06-23 13:55:43,919 : INFO : no word id mapping provided; initializing from corpus\n",
      "2020-06-23 13:55:43,921 : INFO : storing corpus in Blei's LDA-C format into corpus.lda-c\n",
      "2020-06-23 13:55:43,925 : INFO : saving vocabulary of 2 words to corpus.lda-c.vocab\n",
      "2020-06-23 13:55:43,930 : INFO : saving BleiCorpus index to corpus.lda-c.index\n",
      "2020-06-23 13:55:43,935 : INFO : no word id mapping provided; initializing from corpus\n",
      "2020-06-23 13:55:43,938 : INFO : storing corpus in List-Of-Words format into corpus.low\n",
      "2020-06-23 13:55:43,942 : WARNING : List-of-words format can only save vectors with integer elements; 1 float entries were truncated to integer value\n",
      "2020-06-23 13:55:43,946 : INFO : saving LowCorpus index to corpus.low.index\n"
     ]
    }
   ],
   "source": [
    "# Create a toy corpus of 2 documents with one empty document\n",
    "corpus = [[(1, 0.5)], []]\n",
    "\n",
    "# save corpus in the Matrix Market format\n",
    "corpora.MmCorpus.serialize('corpus.mm', corpus)\n",
    "\n",
    "# save corpus in the Joachim's SVMlight format\n",
    "corpora.SvmLightCorpus.serialize('corpus.svmlight', corpus)\n",
    "\n",
    "# save corpus in the Blei's LDA-C format\n",
    "corpora.BleiCorpus.serialize('corpus.lda-c', corpus)\n",
    "\n",
    "# save corpus in the Gibbs LDA++ format\n",
    "corpora.LowCorpus.serialize('corpus.low', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-23 13:55:50,921 : INFO : loaded corpus index from corpus.mm.index\n",
      "2020-06-23 13:55:50,923 : INFO : initializing cython corpus reader from corpus.mm\n",
      "2020-06-23 13:55:50,927 : INFO : accepted corpus with 2 documents, 2 features, 1 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.5)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Load a corpus iterator from file\n",
    "corpus = corpora.MmCorpus('corpus.mm')\n",
    "\n",
    "# Print one document at a time making use of the streaming interface\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility with NumPy and SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random numpy array\n",
    "numpy_matrix = np.random.randint(10, size=(5,2))\n",
    "\n",
    "corpus = gensim.matutils.Dense2Corpus(numpy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sparse matrix\n",
    "scipy_sparse_matrix = scipy.sparse.random(5,2)\n",
    "\n",
    "corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)\n",
    "scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
